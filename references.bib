
@misc{noauthor_distinguishing_2012,
	title = {Distinguishing a {Choroidal} {Nevus} {From} a {Choroidal} {Melanoma}},
	url = {https://www.aao.org/eyenet/article/distinguishing-choroidal-nevus-from-choroidal-mela},
	abstract = {Metastasis of and death from choroidal melanoma have been shown to correlate with increasing basal diameter and increasing thickness of the lesion. Thus, early detection is important. In addition, mak},
	language = {en},
	urldate = {2024-04-21},
	journal = {American Academy of Ophthalmology},
	month = feb,
	year = {2012},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-04-21},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{zhang_generalized_2018,
	title = {Generalized {Cross} {Entropy} {Loss} for {Training} {Deep} {Neural} {Networks} with {Noisy} {Labels}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/f2925f97bc13ad2852a7a551802feea0-Abstract.html},
	abstract = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and large-scale datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.},
	urldate = {2024-04-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Zhilu and Sabuncu, Mert},
	year = {2018},
}

@inproceedings{kaur_automated_2019,
	title = {Automated {Brain} {Image} {Classification} {Based} on {VGG}-16 and {Transfer} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9031952},
	doi = {10.1109/ICIT48102.2019.00023},
	abstract = {The last few decades have witnessed active research in the domain of pathological brain image classification starting from classical to the deep learning approaches like convolutional neural networks(CNN). The classical machine learning methods need hand-crafted features to perform classification. CNN's, on the other hand, performs classification by extracting image features directly from raw images. The features extracted by CNN strongly depends on the training data set size. If the size is small, CNN tends to overfit. So, deep CNN's (DCNN) with transfer learning has evolved. The prime aim of the present paper is to explore the capability of a pre-trained DCNN VGG-16 model with transfer learning for pathological brain image categorization. Only, the last few layers of the VGG-16 model were replaced to accommodate new image categories in the present application. The pre-trained model with transfer learning has been validated on the dataset taken from the Harvard Medical School repository, comprising of normal as well as abnormal MR images with different neurological diseases. The data set was then partitioned using a 10-fold cross-validation mechanism. The validation on the test set using sensitivity (Se), specificity (Sp), and, accuracy (Acc), reveal that the pre-trained VGG-16 model with transfer learning exhibited the best performance in contrast to the other existing state-of-the-art works. Moreover, the approach provides categorization with an end-to-end structure on raw images without any hand-crafted attribute extraction.},
	urldate = {2024-04-21},
	booktitle = {2019 {International} {Conference} on {Information} {Technology} ({ICIT})},
	author = {Kaur, Taranjit and Gandhi, Tapan Kumar},
	month = dec,
	year = {2019},
	keywords = {Brain modeling, Feature extraction, Image classification, Mathematical model, Pathological, VGG-16, Transfer Learning, Classification, Support vector machines, Training},
	pages = {94--98},
}

@article{chaib_deep_2017,
	title = {Deep {Feature} {Fusion} for {VHR} {Remote} {Sensing} {Scene} {Classification}},
	volume = {55},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/document/7934005},
	doi = {10.1109/TGRS.2017.2700322},
	abstract = {The rapid development of remote sensing technology allows us to get images with high and very high resolution (VHR). VHR imagery scene classification has become an important and challenging problem. In this paper, we introduce a framework for VHR scene understanding. First, the pretrained visual geometry group network (VGG-Net) model is proposed as deep feature extractors to extract informative features from the original VHR images. Second, we select the fully connected layers constructed by VGG-Net in which each layer is regarded as separated feature descriptors. And then we combine between them to construct final representation of the VHR image scenes. Third, discriminant correlation analysis (DCA) is adopted as feature fusion strategy to further refine the original features extracting from VGG-Net, which allows a more efficient fusion approach with small cost than the traditional feature fusion strategies. We apply our approach to three challenging data sets: 1) UC MERCED data set that contains 21 different areal scene categories with submeter resolution; 2) WHU-RS data set that contains 19 challenging scene categories with various resolutions; and 3) the Aerial Image data set that has a number of 10 000 images within 30 challenging scene categories with various resolutions. The experimental results demonstrate that our proposed method outperforms the state-of-the-art approaches. Using feature fusion technique achieves a higher accuracy than solely using the raw deep features. Moreover, the proposed method based on DCA fusion produces good informative features to describe the images scene with much lower dimension.},
	number = {8},
	urldate = {2024-04-21},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Chaib, Souleyman and Liu, Huan and Gu, Yanfeng and Yao, Hongxun},
	month = aug,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Correlation, Discriminant correlation analysis (DCA), Feature extraction, Image resolution, Machine learning, Principal component analysis, Remote sensing, Visualization, features fusion, scene classification, unsupervised features learning},
	pages = {4775--4784},
}

@article{hwang_optical_2015,
	title = {Optical {Coherence} {Tomography} {Angiography} {Features} of {Diabetic} {Retinopathy}},
	volume = {35},
	issn = {0275-004X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4623938/},
	doi = {10.1097/IAE.0000000000000716},
	abstract = {Purpose
To describe the optical coherence tomography (OCT) angiography features of diabetic retinopathy

Methods
Using a 70kHz OCT and the split-spectrum amplitude decorrelation angiography (SSADA) algorithm, 6 × 6 mm 3-dimensional angiograms of the macula of 4 patients with diabetic retinopathy were obtained and compared with fluorescein angiography (FA) for features catalogued by the Early Treatment of Diabetic Retinopathy Study.

Results
OCT angiography detected enlargement and distortion of the foveal avascular zone, retinal capillary dropout, and pruning of arteriolar branches. Areas of capillary loss obscured by fluorescein leakage on FA were more clearly defined on OCT angiography. Some areas of focal leakage on FA that were thought to be microaneurysms were found to be small tufts of neovascularization that extended above the inner limiting membrane.

Conclusion
OCT angiography does not show leakage, but can better delineate areas of capillary dropout and detect early retinal neovascularization. This new noninvasive angiography technology may be useful for routine surveillance of proliferative and ischemic changes in diabetic retinopathy.},
	number = {11},
	urldate = {2024-04-21},
	journal = {Retina (Philadelphia, Pa.)},
	author = {Hwang, Thomas S. and Jia, Yali and Gao, Simon S. and Bailey, Steven T. and Lauer, Andreas K. and Flaxel, Christina J. and Wilson, David J. and Huang, David},
	month = nov,
	year = {2015},
	pmid = {26308529},
	pmcid = {PMC4623938},
	pages = {2371--2376},
}

@inproceedings{selvaraju_grad-cam_2017,
	title = {Grad-{CAM}: {Visual} {Explanations} {From} {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	shorttitle = {Grad-{CAM}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html},
	urldate = {2024-04-21},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	year = {2017},
	pages = {618--626},
}

@article{guidotti_survey_2018,
	title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
	volume = {51},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3236009},
	doi = {10.1145/3236009},
	abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	number = {5},
	urldate = {2024-04-21},
	journal = {ACM Computing Surveys},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	month = aug,
	year = {2018},
	keywords = {Open the black box, explanations, interpretability, transparent models},
	pages = {93:1--93:42},
}

@misc{noauthor_diabetic_nodate,
	title = {Diabetic {Retinopathy} {Detection}},
	url = {https://kaggle.com/competitions/diabetic-retinopathy-detection},
	abstract = {Identify signs of diabetic retinopathy in eye images},
	language = {en},
	urldate = {2024-04-21},
}

@misc{noauthor_diabetic_nodate-1,
	title = {Diabetic {Retinopathy} {Detection}},
	url = {https://kaggle.com/competitions/diabetic-retinopathy-detection},
	abstract = {Identify signs of diabetic retinopathy in eye images},
	language = {en},
	urldate = {2024-04-21},
}

@article{hwang_optical_2015-1,
	title = {{OPTICAL} {COHERENCE} {TOMOGRAPHY} {ANGIOGRAPHY} {FEATURES} {OF} {DIABETIC} {RETINOPATHY}},
	volume = {35},
	issn = {0275-004X},
	url = {https://journals.lww.com/retinajournal/fulltext/2015/11000/OPTICAL_COHERENCE_TOMOGRAPHY_ANGIOGRAPHY_FEATURES.26.aspx?casa_token=dh0iQbbEuQYAAAAA:NDAXSJd-CX9Ye1IkFn3-DP89ETRG2jid26MJUUhsPc89xbRfmh2ZDpQnmFlqnOcdwvg-mhCoDDtl8eRKHHd7pQ},
	doi = {10.1097/IAE.0000000000000716},
	abstract = {Purpose: 
          To describe the optical coherence tomography angiography features of diabetic retinopathy.
          Methods: 
          Using a 70 kHz optical coherence tomography and the split-spectrum amplitude decorrelation angiography algorithm, 6 mm × 6 mm 3-dimensional angiograms of the macula of 4 patients with diabetic retinopathy were obtained and compared with fluorescein angiography for features cataloged by the Early Treatment of Diabetic Retinopathy Study.
          Results: 
          Optical coherence tomography angiography detected enlargement and distortion of the foveal avascular zone, retinal capillary dropout, and pruning of arteriolar branches. Areas of capillary loss obscured by fluorescein leakage on fluorescein angiography were more clearly defined on optical coherence tomography angiography. Some areas of focal leakage on fluorescein angiography that were thought to be microaneurysms were found to be small tufts of neovascularization that extended above the inner limiting membrane.
          Conclusion: 
          Optical coherence tomography angiography does not show leakage but can better delineate areas of capillary dropout and detect early retinal neovascularization. This new noninvasive angiography technology may be useful for routine surveillance of proliferative and ischemic changes in diabetic retinopathy.},
	language = {en-US},
	number = {11},
	urldate = {2024-04-21},
	journal = {RETINA},
	author = {Hwang, Thomas S. and Jia, Yali and Gao, Simon S. and Bailey, Steven T. and Lauer, Andreas K. and Flaxel, Christina J. and Wilson, David J. and Huang, David},
	month = nov,
	year = {2015},
	pages = {2371},
}

@article{esteva_deep_2021,
	title = {Deep learning-enabled medical computer vision},
	volume = {4},
	copyright = {2021 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-020-00376-2},
	doi = {10.1038/s41746-020-00376-2},
	abstract = {A decade of unprecedented progress in artificial intelligence (AI) has demonstrated the potential for many fields—including medicine—to benefit from the insights that AI techniques can extract from data. Here we survey recent progress in the development of modern computer vision techniques—powered by deep learning—for medical applications, focusing on medical imaging, medical video, and clinical deployment. We start by briefly summarizing a decade of progress in convolutional neural networks, including the vision tasks they enable, in the context of healthcare. Next, we discuss several example medical imaging applications that stand to benefit—including cardiology, pathology, dermatology, ophthalmology–and propose new avenues for continued work. We then expand into general medical video, highlighting ways in which clinical workflows can integrate computer vision to enhance care. Finally, we discuss the challenges and hurdles required for real-world clinical deployment of these technologies.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {npj Digital Medicine},
	author = {Esteva, Andre and Chou, Katherine and Yeung, Serena and Naik, Nikhil and Madani, Ali and Mottaghi, Ali and Liu, Yun and Topol, Eric and Dean, Jeff and Socher, Richard},
	month = jan,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Health care, Medical research},
	pages = {1--9},
}

@book{tammina_transfer_2019,
	title = {Transfer learning using {VGG}-16 with {Deep} {Convolutional} {Neural} {Network} for {Classifying} {Images}},
	volume = {9},
	abstract = {Traditionally, data mining algorithms and machine learning algorithms are engineered to approach the problems in isolation. These algorithms are employed to train the model in separation on a specific feature space and same distribution. Depending on the business case, a model is trained by applying a machine learning algorithm for a specific task. A widespread assumption in the field of machine learning is that training data and test data must have identical feature spaces with the underlying distribution. On the contrary, in real world this assumption may not hold and thus models need to be rebuilt from the scratch if features and distribution changes. It is an arduous process to collect related training data and rebuild the models. In such cases, Transferring of Knowledge or transfer learning from disparate domains would be desirable. Transfer learning is a method of reusing a pre-trained model knowledge for another task. Transfer learning can be used for classification, regression and clustering problems. This paper uses one of the pre-trained models – VGG - 16 with Deep Convolutional Neural Network to classify images.},
	author = {Tammina, Srikanth},
	month = oct,
	year = {2019},
	doi = {10.29322/IJSRP.9.10.2019.p9420},
	note = {Journal Abbreviation: International Journal of Scientific and Research Publications (IJSRP)
Publication Title: International Journal of Scientific and Research Publications (IJSRP)},
}

@article{majib_vgg-scnet_2021,
	title = {{VGG}-{SCNet}: {A} {VGG} {Net}-{Based} {Deep} {Learning} {Framework} for {Brain} {Tumor} {Detection} on {MRI} {Images}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {{VGG}-{SCNet}},
	url = {https://ieeexplore.ieee.org/abstract/document/9515947},
	doi = {10.1109/ACCESS.2021.3105874},
	abstract = {A brain tumor is a life-threatening neurological condition caused by the unregulated development of cells inside the brain or skull. The death rate of people with this condition is steadily increasing. Early diagnosis of malignant tumors is critical for providing treatment to patients, and early discovery improves the patient’s chances of survival. The patient’s survival rate is usually very less if they are not adequately treated. If a brain tumor cannot be identified in an early stage, it can surely lead to death. Therefore, early diagnosis of brain tumors necessitates the use of an automated tool. The segmentation, diagnosis, and isolation of contaminated tumor areas from magnetic resonance (MR) images is a prime concern. However, it is a tedious and time-consuming process that radiologists or clinical specialists must undertake, and their performance is solely dependent on their expertise. To address these limitations, the use of computer-assisted techniques becomes critical. In this paper, different traditional and hybrid ML models were built and analyzed in detail to classify the brain tumor images without any human intervention. Along with these, 16 different transfer learning models were also analyzed to identify the best transfer learning model to classify brain tumors based on neural networks. Finally, using different state-of-the-art technologies, a stacked classifier was proposed which outperforms all the other developed models. The proposed VGG-SCNet’s (VGG Stacked Classifier Network) precision, recall, and f1 scores were found to be 99.2\%, 99.1\%, and 99.2\% respectively.},
	urldate = {2024-04-21},
	journal = {IEEE Access},
	author = {Majib, Mohammad Shahjahan and Rahman, Md. Mahbubur and Sazzad, T. M. Shahriar and Khan, Nafiz Imtiaz and Dey, Samrat Kumar},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Brain modeling, Brain tumor, Convolutional neural networks, Deep learning, Feature extraction, MRI, Machine learning, Magnetic resonance imaging, Tumors, deep learning, detection, machine learning, prediction},
	pages = {116942--116952},
}

@article{lu_feature_2017,
	title = {Feature {Extraction} and {Fusion} {Using} {Deep} {Convolutional} {Neural} {Networks} for {Face} {Detection}},
	volume = {2017},
	issn = {1024-123X},
	url = {https://www.hindawi.com/journals/mpe/2017/1376726/},
	doi = {10.1155/2017/1376726},
	abstract = {This paper proposes a method that uses feature fusion to represent images better for face detection after feature extraction by deep convolutional neural network (DCNN). First, with Clarifai net and VGG Net-D (16 layers), we learn features from data, respectively; then we fuse features extracted from the two nets. To obtain more compact feature representation and mitigate computation complexity, we reduce the dimension of the fused features by PCA. Finally, we conduct face classification by SVM classifier for binary classification. In particular, we exploit offset max-pooling to extract features with sliding window densely, which leads to better matches of faces and detection windows; thus the detection result is more accurate. Experimental results show that our method can detect faces with severe occlusion and large variations in pose and scale. In particular, our method achieves 89.24\% recall rate on FDDB and 97.19\% average precision on AFW.},
	language = {en},
	urldate = {2024-04-21},
	journal = {Mathematical Problems in Engineering},
	author = {Lu, Xiaojun and Duan, Xu and Mao, Xiuping and Li, Yuanyuan and Zhang, Xiangde},
	month = jan,
	year = {2017},
	note = {Publisher: Hindawi},
	pages = {e1376726},
}

@article{chaib_deep_2017-1,
	title = {Deep {Feature} {Fusion} for {VHR} {Remote} {Sensing} {Scene} {Classification}},
	volume = {55},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/abstract/document/7934005?casa_token=fx2H5lJ6e-YAAAAA:bKG_ORNy82UEysZF5evkjlGQuRnCkmZuoiKe741ui7vwv4Q1uljgOf4EHkQYfWeqsu0MUzxQgv8},
	doi = {10.1109/TGRS.2017.2700322},
	abstract = {The rapid development of remote sensing technology allows us to get images with high and very high resolution (VHR). VHR imagery scene classification has become an important and challenging problem. In this paper, we introduce a framework for VHR scene understanding. First, the pretrained visual geometry group network (VGG-Net) model is proposed as deep feature extractors to extract informative features from the original VHR images. Second, we select the fully connected layers constructed by VGG-Net in which each layer is regarded as separated feature descriptors. And then we combine between them to construct final representation of the VHR image scenes. Third, discriminant correlation analysis (DCA) is adopted as feature fusion strategy to further refine the original features extracting from VGG-Net, which allows a more efficient fusion approach with small cost than the traditional feature fusion strategies. We apply our approach to three challenging data sets: 1) UC MERCED data set that contains 21 different areal scene categories with submeter resolution; 2) WHU-RS data set that contains 19 challenging scene categories with various resolutions; and 3) the Aerial Image data set that has a number of 10 000 images within 30 challenging scene categories with various resolutions. The experimental results demonstrate that our proposed method outperforms the state-of-the-art approaches. Using feature fusion technique achieves a higher accuracy than solely using the raw deep features. Moreover, the proposed method based on DCA fusion produces good informative features to describe the images scene with much lower dimension.},
	number = {8},
	urldate = {2024-04-21},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Chaib, Souleyman and Liu, Huan and Gu, Yanfeng and Yao, Hongxun},
	month = aug,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Correlation, Discriminant correlation analysis (DCA), Feature extraction, Image resolution, Machine learning, Principal component analysis, Remote sensing, Visualization, features fusion, scene classification, unsupervised features learning},
	pages = {4775--4784},
}

@inproceedings{varshni_pneumonia_2019,
	title = {Pneumonia {Detection} {Using} {CNN} based {Feature} {Extraction}},
	url = {https://ieeexplore.ieee.org/abstract/document/8869364},
	doi = {10.1109/ICECCT.2019.8869364},
	abstract = {Pneumonia is a life-threatening infectious disease affecting one or both lungs in humans commonly caused by bacteria called Streptococcus pneumoniae. One in three deaths in India is caused due to pneumonia as reported by World Health Organization (WHO). Chest X-Rays which are used to diagnose pneumonia need expert radiotherapists for evaluation. Thus, developing an automatic system for detecting pneumonia would be beneficial for treating the disease without any delay particularly in remote areas. Due to the success of deep learning algorithms in analyzing medical images, Convolutional Neural Networks (CNNs) have gained much attention for disease classification. In addition, features learned by pre-trained CNN models on large-scale datasets are much useful in image classification tasks. In this work, we appraise the functionality of pre-trained CNN models utilized as feature-extractors followed by different classifiers for the classification of abnormal and normal chest X-Rays. We analytically determine the optimal CNN model for the purpose. Statistical results obtained demonstrates that pretrained CNN models employed along with supervised classifier algorithms can be very beneficial in analyzing chest X-ray images, specifically to detect Pneumonia.},
	urldate = {2024-04-21},
	booktitle = {2019 {IEEE} {International} {Conference} on {Electrical}, {Computer} and {Communication} {Technologies} ({ICECCT})},
	author = {Varshni, Dimpy and Thakral, Kartik and Agarwal, Lucky and Nijhawan, Rahul and Mittal, Ankush},
	month = feb,
	year = {2019},
	keywords = {Convolutional neural networks, Deep Convolutional Neural Networks, DensetNet, Diseases, Feature extraction, K-nearest neighbors, Lung, Naive Bayes, Random Forest, SVM, Support vector machines, Task analysis, Transfer Learning, X-rays},
	pages = {1--7},
}

@article{ha_image_2018,
	title = {Image retrieval using {BIM} and features from pretrained {VGG} network for indoor localization},
	volume = {140},
	issn = {0360-1323},
	url = {https://www.sciencedirect.com/science/article/pii/S0360132318302865},
	doi = {10.1016/j.buildenv.2018.05.026},
	abstract = {Various devices that are used indoors require information regarding the user's position and orientation. This information enables the devices to offer the user customized and more relevant information. This study presents a new image-based indoor localization method using building information modeling (BIM) and convolutional neural networks (CNNs). This method constructs a dataset with rendered BIM images and searches the dataset for images most similar to indoor photographs, thereby estimating the indoor position and orientation of the photograph. A pretrained CNN (the VGG network) is used for image feature extraction for the similarity evaluation of two different types of images (BIM rendered and real images). Experiments were performed in real buildings to verify the method, and the matching accuracy is 91.61\% for a total of 143 images. The results also confirm that pooling layer 4 in the VGG network is best suited for feature selection.},
	urldate = {2024-04-21},
	journal = {Building and Environment},
	author = {Ha, Inhae and Kim, Hongjo and Park, Somin and Kim, Hyoungkwan},
	month = aug,
	year = {2018},
	keywords = {BIM, Cross-domain image retrieval, Feature extraction, Global descriptor, Image-based indoor localization},
	pages = {23--31},
}

@inproceedings{kaur_automated_2019-1,
	title = {Automated {Brain} {Image} {Classification} {Based} on {VGG}-16 and {Transfer} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9031952?casa_token=kj0FvG6m68cAAAAA:W-7ImF0J_AaRrd0IP7jip6unYTYSruQCnwebb5hnfZFxjSTODNDbIsy3aqYDvtJST03s_Zc5qAI},
	doi = {10.1109/ICIT48102.2019.00023},
	abstract = {The last few decades have witnessed active research in the domain of pathological brain image classification starting from classical to the deep learning approaches like convolutional neural networks(CNN). The classical machine learning methods need hand-crafted features to perform classification. CNN's, on the other hand, performs classification by extracting image features directly from raw images. The features extracted by CNN strongly depends on the training data set size. If the size is small, CNN tends to overfit. So, deep CNN's (DCNN) with transfer learning has evolved. The prime aim of the present paper is to explore the capability of a pre-trained DCNN VGG-16 model with transfer learning for pathological brain image categorization. Only, the last few layers of the VGG-16 model were replaced to accommodate new image categories in the present application. The pre-trained model with transfer learning has been validated on the dataset taken from the Harvard Medical School repository, comprising of normal as well as abnormal MR images with different neurological diseases. The data set was then partitioned using a 10-fold cross-validation mechanism. The validation on the test set using sensitivity (Se), specificity (Sp), and, accuracy (Acc), reveal that the pre-trained VGG-16 model with transfer learning exhibited the best performance in contrast to the other existing state-of-the-art works. Moreover, the approach provides categorization with an end-to-end structure on raw images without any hand-crafted attribute extraction.},
	urldate = {2024-04-21},
	booktitle = {2019 {International} {Conference} on {Information} {Technology} ({ICIT})},
	author = {Kaur, Taranjit and Gandhi, Tapan Kumar},
	month = dec,
	year = {2019},
	keywords = {Brain modeling, Feature extraction, Image classification, Mathematical model, Pathological, VGG-16, Transfer Learning, Classification, Support vector machines, Training},
	pages = {94--98},
}

@article{mateen_fundus_2019,
	title = {Fundus {Image} {Classification} {Using} {VGG}-19 {Architecture} with {PCA} and {SVD}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/11/1/1},
	doi = {10.3390/sym11010001},
	abstract = {Automated medical image analysis is an emerging field of research that identifies the disease with the help of imaging technology. Diabetic retinopathy (DR) is a retinal disease that is diagnosed in diabetic patients. Deep neural network (DNN) is widely used to classify diabetic retinopathy from fundus images collected from suspected persons. The proposed DR classification system achieves a symmetrically optimized solution through the combination of a Gaussian mixture model (GMM), visual geometry group network (VGGNet), singular value decomposition (SVD) and principle component analysis (PCA), and softmax, for region segmentation, high dimensional feature extraction, feature selection and fundus image classification, respectively. The experiments were performed using a standard KAGGLE dataset containing 35,126 images. The proposed VGG-19 DNN based DR model outperformed the AlexNet and spatial invariant feature transform (SIFT) in terms of classification accuracy and computational time. Utilization of PCA and SVD feature selection with fully connected (FC) layers demonstrated the classification accuracies of 92.21\%, 98.34\%, 97.96\%, and 98.13\% for FC7-PCA, FC7-SVD, FC8-PCA, and FC8-SVD, respectively.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {Symmetry},
	author = {Mateen, Muhammad and Wen, Junhao and Nasrullah and Song, Sun and Huang, Zhouping},
	month = jan,
	year = {2019},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {PCA, SVD, VGGNet DNN, deep convolutional neural network, diabetic retinopathy, fundus images},
	pages = {1},
}

@article{yun_identification_2008,
	title = {Identification of different stages of diabetic retinopathy using retinal optical images},
	volume = {178},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025507003635},
	doi = {10.1016/j.ins.2007.07.020},
	abstract = {Diabetes is a disease which occurs when the pancreas does not secrete enough insulin or the body is unable to process it properly. This disease affects slowly the circulatory system including that of the retina. As diabetes progresses, the vision of a patient may start to deteriorate and lead to diabetic retinopathy. In this study on different stages of diabetic retinopathy, 124 retinal photographs were analyzed. As a result, four groups were identified, viz., normal retina, moderate non-proliferative diabetic retinopathy, severe non-proliferative diabetic retinopathy and proliferative diabetic retinopathy. Classification of the four eye diseases was achieved using a three-layer feedforward neural network. The features are extracted from the raw images using the image processing techniques and fed to the classifier for classification. We demonstrate a sensitivity of more than 90\% for the classifier with the specificity of 100\%.},
	number = {1},
	urldate = {2024-04-21},
	journal = {Information Sciences},
	author = {Yun, Wong Li and Rajendra Acharya, U. and Venkatesh, Y. V. and Chee, Caroline and Min, Lim Choo and Ng, E. Y. K.},
	month = jan,
	year = {2008},
	keywords = {Classification, Eye, Features, Feedforward, Image processing, Neural network, Normal, Retinopathy},
	pages = {106--121},
}

@article{lee_epidemiology_2015,
	title = {Epidemiology of diabetic retinopathy, diabetic macular edema and related vision loss},
	volume = {2},
	issn = {2326-0254},
	url = {https://doi.org/10.1186/s40662-015-0026-2},
	doi = {10.1186/s40662-015-0026-2},
	abstract = {Diabetic retinopathy (DR) is a leading cause of vision-loss globally. Of an estimated 285 million people with diabetes mellitus worldwide, approximately one third have signs of DR and of these, a further one third of DR is vision-threatening DR, including diabetic macular edema (DME). The identification of established modifiable risk factors for DR such as hyperglycemia and hypertension has provided the basis for risk factor control in preventing onset and progression of DR. Additional research investigating novel risk factors has improved our understanding of multiple biological pathways involved in the pathogenesis of DR and DME, especially those involved in inflammation and oxidative stress. Variations in DR prevalence between populations have also sparked interest in genetic studies to identify loci associated with disease susceptibility. In this review, major trends in the prevalence, incidence, progression and regression of DR and DME are explored, and gaps in literature identified. Established and novel risk factors are also extensively reviewed with a focus on landmark studies and updates from the recent literature.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {Eye and Vision},
	author = {Lee, Ryan and Wong, Tien Y. and Sabanayagam, Charumathi},
	month = sep,
	year = {2015},
	keywords = {Diabetic macular edema, Diabetic retinopathy, Epidemiology, Risk factors},
	pages = {17},
}

@article{hammes_diabetic_2011,
	title = {Diabetic {Retinopathy}: {Targeting} {Vasoregression}},
	volume = {60},
	issn = {0012-1797},
	shorttitle = {Diabetic {Retinopathy}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3012202/},
	doi = {10.2337/db10-0454},
	number = {1},
	urldate = {2024-04-21},
	journal = {Diabetes},
	author = {Hammes, Hans-Peter and Feng, Yuxi and Pfister, Frederick and Brownlee, Michael},
	month = jan,
	year = {2011},
	pmid = {21193734},
	pmcid = {PMC3012202},
	pages = {9--16},
}

@article{stitt_progress_2016,
	title = {The progress in understanding and treatment of diabetic retinopathy},
	volume = {51},
	issn = {1350-9462},
	url = {https://www.sciencedirect.com/science/article/pii/S135094621500066X},
	doi = {10.1016/j.preteyeres.2015.08.001},
	abstract = {Diabetic retinopathy is the most frequently occurring complication of diabetes mellitus and remains a leading cause of vision loss globally. Its aetiology and pathology have been extensively studied for half a century, yet there are disappointingly few therapeutic options. Although some new treatments have been introduced for diabetic macular oedema (DMO) (e.g. intravitreal vascular endothelial growth factor inhibitors (‘anti-VEGFs’) and new steroids), up to 50\% of patients fail to respond. Furthermore, for people with proliferative diabetic retinopathy (PDR), laser photocoagulation remains a mainstay therapy, even though it is an inherently destructive procedure. This review summarises the clinical features of diabetic retinopathy and its risk factors. It describes details of retinal pathology and how advances in our understanding of pathogenesis have led to identification of new therapeutic targets. We emphasise that although there have been significant advances, there is still a pressing need for a better understanding basic mechanisms enable development of reliable and robust means to identify patients at highest risk, and to intervene effectively before vision loss occurs.},
	urldate = {2024-04-21},
	journal = {Progress in Retinal and Eye Research},
	author = {Stitt, Alan W. and Curtis, Timothy M. and Chen, Mei and Medina, Reinhold J. and McKay, Gareth J. and Jenkins, Alicia and Gardiner, Thomas A. and Lyons, Timothy J. and Hammes, Hans-Peter and Simó, Rafael and Lois, Noemi},
	month = mar,
	year = {2016},
	keywords = {Diabetes, Diabetic macular oedema, Diabetic retinopathy, Pathogenesis, Retina},
	pages = {156--186},
}

@misc{noauthor_diabetic_2023,
	title = {Diabetic {Retinopathy} {Estimates} {\textbar} {Vision} and {Eye} {Health} {Surveillance} {System} {\textbar} {CDC}},
	url = {https://www.cdc.gov/visionhealth/vehss/estimates/dr-prevalence.html},
	abstract = {Prevalence of Diabetic Retinopathy (DR) from 2021 survey data includes overall findings, group and county graphs.},
	language = {en-us},
	urldate = {2024-04-20},
	month = jul,
	year = {2023},
}

@misc{noauthor_diabetic_nodate-2,
	title = {Diabetic {Retinopathy} {\textbar} {National} {Eye} {Institute}},
	url = {https://www.nei.nih.gov/learn-about-eye-health/eye-conditions-and-diseases/diabetic-retinopathy},
	urldate = {2024-04-20},
}

@article{amann_explainability_2020,
	title = {Explainability for artificial intelligence in healthcare: a multidisciplinary perspective},
	volume = {20},
	issn = {1472-6947},
	shorttitle = {Explainability for artificial intelligence in healthcare},
	url = {https://doi.org/10.1186/s12911-020-01332-6},
	doi = {10.1186/s12911-020-01332-6},
	abstract = {Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice.},
	language = {en},
	number = {1},
	urldate = {2024-04-20},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I. and {the Precise4Q consortium}},
	month = nov,
	year = {2020},
	keywords = {Artificial intelligence, Clinical decision support, Explainability, Interpretability, Machine learning},
	pages = {310},
}

@article{selvaraju_grad-cam_nodate,
	title = {Grad-{CAM}: {Visual} {Explanations} {From} {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), ﬂowing into the ﬁnal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing ﬁne-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classiﬁcation, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classiﬁcation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]1 and video at youtu.be/COjUB9Izk6E.},
	language = {en},
	author = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
}

@misc{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv:1312.6034 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	language = {en},
	number = {11},
	urldate = {2024-04-20},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
}

@misc{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kollias_diabetic_2010,
	title = {Diabetic {Retinopathy}},
	volume = {107},
	issn = {1866-0452},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2828250/},
	doi = {10.3238/arztebl.2010.0075},
	abstract = {Background
Diabetic retinopathy is a microangiopathy of the retina from which nearly all persons with diabetes eventually suffer. Two of its complications threaten the patient’s vision: diabetic macular edema and proliferative diabetic retinopathy.

Methods
Selective literature review, based on national and international guidelines and a literature search from 1981 onward.

Results
Diabetic retinopathy is subdivided into non-proliferative and proliferative retinopathy. Macular edema can arise at any stage of the disease and threatens visual acuity. The main risk factors for the development and progression of diabetic retinopathy are long duration of diabetes and poor control of blood sugar and arterial blood pressure. Laser photocoagulation is an evidence-based treatment for proliferative retinopathy and macular edema. Vitreous surgery is indicated in cases of worsening vision due to a non-clearing vitreous hemorrhage or tractional retinal detachment. The current options for medical treatment involve the intravitreous injection of glucocorticosteroids or of a VEGF antagonist; both of these options are “off label” at present.

Conclusion
Diabetic retinopathy is the leading cause of blindness among persons of working age in the industrialized world. Regular ophthalmological examinations, timely laser therapy depending on the stage of the disease, and close interdisciplinary cooperation are essential to prevent loss of vision.},
	number = {5},
	urldate = {2024-04-20},
	journal = {Deutsches Arzteblatt International},
	author = {Kollias, Aris N. and Ulbig, Michael W.},
	month = feb,
	year = {2010},
	pmid = {20186318},
	pmcid = {PMC2828250},
	pages = {75--84},
}

@article{acharya_integrated_2012,
	title = {An integrated index for the identification of diabetic retinopathy stages using texture parameters},
	volume = {36},
	issn = {0148-5598},
	doi = {10.1007/s10916-011-9663-8},
	abstract = {Diabetes is a condition of increase in the blood sugar level higher than the normal range. Prolonged diabetes damages the small blood vessels in the retina resulting in diabetic retinopathy (DR). DR progresses with time without any noticeable symptoms until the damage has occurred. Hence, it is very beneficial to have the regular cost effective eye screening for the diabetes subjects. This paper documents a system that can be used for automatic mass screenings of diabetic retinopathy. Four classes are identified: normal retina, non-proliferative diabetic retinopathy (NPDR), proliferative diabetic retinopathy (PDR), and macular edema (ME). We used 238 retinal fundus images in our analysis. Five different texture features such as homogeneity, correlation, short run emphasis, long run emphasis, and run percentage were extracted from the digital fundus images. These features were fed into a support vector machine classifier (SVM) for automatic classification. SVM classifier of different kernel functions (linear, radial basis function, polynomial of order 1, 2, and 3) was studied. Receiver operation characteristics (ROC) curves were plotted to select the best classifier. Our proposed system is able to identify the unknown class with an accuracy of 85.2\%, and sensitivity, specificity, and area under curve (AUC) of 98.9\%, 89.5\%, and 0.972 respectively using SVM classifier with polynomial kernel of order 3. We have also proposed a new integrated DR index (IDRI) using different features, which is able to identify the different classes with 100\% accuracy.},
	language = {eng},
	number = {3},
	journal = {Journal of Medical Systems},
	author = {Acharya, U. Rajendra and Ng, E. Y. K. and Tan, Jen-Hong and Sree, S. Vinitha and Ng, Kwan-Hoong},
	month = jun,
	year = {2012},
	pmid = {21340703},
	keywords = {Diabetic Retinopathy, Diagnosis, Computer-Assisted, Humans, Image Interpretation, Computer-Assisted, Support Vector Machine},
	pages = {2011--2020},
}

@article{hwang_optical_2015-2,
	title = {Optical {Coherence} {Tomography} {Angiography} {Features} of {Diabetic} {Retinopathy}},
	volume = {35},
	issn = {0275-004X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4623938/},
	doi = {10.1097/IAE.0000000000000716},
	abstract = {Purpose
To describe the optical coherence tomography (OCT) angiography features of diabetic retinopathy

Methods
Using a 70kHz OCT and the split-spectrum amplitude decorrelation angiography (SSADA) algorithm, 6 × 6 mm 3-dimensional angiograms of the macula of 4 patients with diabetic retinopathy were obtained and compared with fluorescein angiography (FA) for features catalogued by the Early Treatment of Diabetic Retinopathy Study.

Results
OCT angiography detected enlargement and distortion of the foveal avascular zone, retinal capillary dropout, and pruning of arteriolar branches. Areas of capillary loss obscured by fluorescein leakage on FA were more clearly defined on OCT angiography. Some areas of focal leakage on FA that were thought to be microaneurysms were found to be small tufts of neovascularization that extended above the inner limiting membrane.

Conclusion
OCT angiography does not show leakage, but can better delineate areas of capillary dropout and detect early retinal neovascularization. This new noninvasive angiography technology may be useful for routine surveillance of proliferative and ischemic changes in diabetic retinopathy.},
	number = {11},
	urldate = {2024-02-29},
	journal = {Retina (Philadelphia, Pa.)},
	author = {Hwang, Thomas S. and Jia, Yali and Gao, Simon S. and Bailey, Steven T. and Lauer, Andreas K. and Flaxel, Christina J. and Wilson, David J. and Huang, David},
	month = nov,
	year = {2015},
	pmid = {26308529},
	pmcid = {PMC4623938},
	pages = {2371--2376},
}

@article{staal_ridge-based_2004,
	title = {Ridge-based vessel segmentation in color images of the retina},
	volume = {23},
	doi = {10.1109/TMI.2004.825627},
	number = {4},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Staal, J. and Abramoff, M.D. and Niemeijer, M. and Viergever, M.A. and van Ginneken, B.},
	year = {2004},
	keywords = {Color, Diabetes, Image analysis, Image databases, Image segmentation, Pixel, Retina, Retinopathy, Spatial databases, Testing},
	pages = {501--509},
}

@misc{emma_dugas_diabetic_2015,
	title = {Diabetic {Retinopathy} {Detection}},
	url = {https://kaggle.com/competitions/diabetic-retinopathy-detection},
	publisher = {Kaggle},
	author = {Emma Dugas, Jared, Jorge, Will Cukierski},
	year = {2015},
}
